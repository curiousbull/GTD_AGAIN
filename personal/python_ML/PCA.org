#+TITLE: 主成分分析 (Principle Compnets Analysis, PCA)
#+AUTHOR: 胡琛

* 场景

  在拿到一个数据集的时候，有时候会遇到一些尴尬，譬如数据中，某些特征是重复的，
  或者特征是无关紧要的，有时候则是数据点在高维情况下会遇到稀疏性问题，譬如：

  1. 拿到一个关于汽车性能的数据集，其中对于汽车速度的表述，既有 “公里/时” 的
     表述，又有 “英里/时” 的特征，无疑，我们只需要其中一个特征即可。
  2. 拿到数学系本科生期末成绩表和他们对于数学的感兴趣程度，平均在数学上每周
     花费的时间，很显然，他们对于数学的感兴趣程度和在数学上花费的时间是强相关
     的，而在数学上花费的时间又与他们的数学期末成绩强相关，此时，将他们对
     数学感兴趣程度和在数学上花费时间合并起来是不是会更好。
  3. 拿到某个样本，特征很多，但是样例却非常少，此时，直接用回归去做拟合很容易
     造成过拟合，譬如，北京的房价与房子的朝向、楼层、建造年代，是否学区房，
     是否二手，大小，位置等有关系，然而我们只有十个房子的样例，直接去做回归
     无疑是不合适的。
  4. 譬如在建立的文档-词项矩阵中，出现了 'study' 和 'learn' 这样的两个
     词项，在传统向量空间中，这两个词项无疑是独立的，但是在语义上则是相似
     的，出现频率也近似。
  5. 信号传输过程中，由于信道不理想，信道另一端收到的信号会有扰动，如何去过滤
     噪音：
     - 剔除和类标签无关的特征，譬如 “学生的名字” 与 “学生的成绩” 无关
     - 剔除和类标签有关但里面存在噪声或冗余的特征，在这种情况下，需要一种
       特征降维的方法来降低特征数，减少噪声和冗余，减少过拟合的可能。

* PCA 的思想

  将 $n$ 维特征映射到 $k(k < n)$ 维特征上，这 $k$ 维是全新的正交特征。这 $k$ 
  维特征称为主元，是重新构造出来的 $k$ 维特征，而不是简单地从 $n$ 维特征中去除
  $n-k$ 维特征得到。

  #+BEGIN_QUOTE
  算法思想：最大方差理论，最小平方误差理论，坐标轴相关度理论
  #+END_QUOTE

* 理论基础

** 最大方差理论

   在信号处理中，认为信号具有较大方差，噪声具有较小方差，信噪比就是信号与噪声的
   方差比，越大越好。

   如下图所示，样本在横轴上投影方差相比在纵轴上投影的方差较大，那么，认为在纵轴
   上的投影是由噪声引起的。因此，我们认为，最好的 $k$ 维特征，应该是将 $n$ 维
   样本点转为 $k$ 维样本点后，每一维上的投影方差都很大。

   [[file:PCA/min_var.png]]

   如前所述，在下图中，根据方差最大化理论，显然应该是选左边直线做投影更好。

   [[file:PCA/min_var_which_better.png]]

*** 投影

    [[file:PCA/min_var_proj.png]]

    *最佳的投影向量，应该使得投影后的样本点方差最大*

    在上图中，已知均值维 0，因此方差为：

    #+NAME: eq:pca_01
    #+BEGIN_SRC latex :exports results
      \begin{equation}
        \begin{array}{lcl}
          \frac{1}{m}\sum\limits^m_{i=1}(x^{(i)}u)^2 &=& \frac{1}{m}\sum\limits^m_{i=1}
          (u^T{x^{(i)}}^Tx^{(i)}u)\\
          &=&u^T(\frac{1}{m}\sum\limits^m_{i=1}{x^{(i)}}^Tx^{(i)})u
          \end{array}
      \end{equation}
    #+END_SRC

    其中， $m$ 表示数据集中数据点的个数， $u$ 表示某个投影轴，我们的目的是要找出足够的
    投影轴，譬如 $k(k<n)$ 个投影轴，并以此作为新的坐标轴来表示数据点，由于 $k<n$, 显然
    此时伴随着信息的丢失，因此我们需要取方差最大的组合，以尽可能地保留信息，丢弃噪声等
    冗余信息。

    按之前的讨论，此时应该对 [[eq:pca_01]] 取极大，为此，对该公式进行一些处理。由于 $u$ 是
    单位向量，满足 $u^Tu=1$, 因此，

      \begin{equation}
        \begin{array}{lcl}
          u\frac{1}{m}\sum\limits^m_{i=1}(x^{(i)}u)^2 &=& uu^T\frac{1}{m}
          (\sum\limits^m_{i=1}{x^{(i)}}^Tx^{(i)})u\\
          &=&(\frac{1}{m}
          \sum\limits^m_{i=1}{x^{(i)}}^Tx^{(i)})u
        \end{array}
      \end{equation}

    如果令 $\frac{1}{m}\sum\limits^m_{i=1}(x^{(i)}u)^2 = \lambda$, 
    $(\frac{1}{m}\sum\limits^m_{i=1}{x^{(i)}}^Tx^{(i)})=\Sigma$, 于是，有

    #+BEGIN_SRC latex :exports results
      \begin{equation}
        \lambda u = \Sigma u
      \end{equation}
    #+END_SRC

    $\lambda$ 是 $\Sigma$ 的本征值， $u$ 是特征向量，最佳的投影向量，显然需要 $\lambda$ 取
    最大的时候对应的 $u$. 因此，PCA 就是对 $\Sigma$, 也就是协方差矩阵，求本征值，并按从大到小
    排列，取比较大的 $k$ 个本征值对应的本征向量组成新的 $n\times{}k$ 矩阵空间就是新的表示空间。 

** 最小平方误差理论

   [[file:PCA/least_sq_err.png]]
    
   如果有类似上图的二维样本点 (红色点), 通过线性回归求一个线性函数，使得直线能够最佳拟合样本点，
   回归时， *最小二乘法度量的是样本点到直线的坐标距离*
    
   在此例中，特征是 $x$, 标签是 $y$, 如果使用回归方法来度量最佳直线，那么就是直接在原始样本上
   做回归，和特征选择就没有什么关系，为此，我们选择另一种评价直线好坏的方法，使用点到直线的距离
   $d^{\prime}$ 来度量。
   
   现有 $n$ 个样本点 $(x_1,x_2,\ldots,x_n)$, 每个样本点均为 $m$ 维，将样本点在直线上投影记为，
   
   #+BEGIN_SRC latex :exports results 
     \begin{equation}
       \sum\limits^n_{k=1}||(x_k^{\prime}-x_k)||^2
     \end{equation}
   #+END_SRC
   
   这个公式成为最小平方误差理论 (Least Squared Error), 而确定一条直线，一般只需要确定一个点和
   确定方向即可。

*** 确定点
    
    1. 首先，对于 $(x_1,x_2,\ldots,x_n)$ 这些点，先确定出一个代表点，最简单的方式就是用这些点
       的平均作为代表，即 $x_0 = \frac{1}{n}\sum\limits^n_{k=1}x_k.
       (*直觉而言，对于这些点的分割直线，必然会穿过这些点的重心*)
    2. 令 $x_0 = m$, 如果令直线的单位向量为 $e$, 那么，相应直线上任意一点，可以表示为
       
       #+BEGIN_SRC latex :exports results
         \begin{equation}
           x^{\prime}_k = m + a_ke
         \end{equation}
       #+END_SRC
       
       其中， $a_k$ 是 $x^{\prime}_k$ 到点 $m$ 的距离。

       我们需要做的是，对最小平方误差取最小，

       #+BEGIN_SRC latex :exports results
         \begin{equation}
           \begin{array}{lcl}
             arg\min\limits_{a_1,\ldots,a_n,e}J_1(a_1,\ldots,a_n,e) &=&
             arg\min\limits_{a_1,\ldots,a_n,e}  \sum\limits^n_{k=1}||(m+a_ke)-x_k||^2\\
                                                                    &=&
             arg\min\limits_{a_1,\ldots,a_n,e}\sum\limits^n_{k=1}a_k^2||e||^2-2\sum\limits^n_{k=1}
                         a_ke^T(x_k-m)+\sum\limits^n_{k=1}||x_k-m||^2
             \end{array}
         \end{equation}
       #+END_SRC
    
       求上式对 $a_k$ 的偏导，且有 $||e||^2 = 1$, 因此有

       #+BEGIN_SRC latex :exports results
         \begin{equation}
           a_k = e^t(x_k-m)
         \end{equation}
       #+END_SRC

       然后对 $e$ 求偏导，首先将上式代入 $J_1$, 有：

       #+BEGIN_SRC latex :exports results
         \begin{equation}
           \begin{array}{lcl}
             J_1(e) &=&\sum\limits^n_{k=1}a_k^2||e||^2-2\sum\limits^n_{k=1}a_k^2
                        +\sum\limits^n_{k=1}||x_k-m||^2\\
                    &=&\sum\limits^n_{k=1}e^T(x_k-m)(x_k-m)^Te+\sum\limits^n_{k=1}||x_k-m||^2\\
                    &=&-e^TSe+\sum\limits^n_{k=1}||x_k-m||^2
           \end{array}
         \end{equation}
       #+END_SRC
    
       其中， $S=\sum\limits^n_{k=1}e^T(x_k-m)(x_k-m)^Te$ 称为散列矩阵，然后对 $e$ 求偏导，其中 $||e||^2=1$,
       引入拉格朗日乘子 $\lambda$, 令 $u=e^TSe-\lambda(e^Te-1)$, 求偏导，有

       #+BEGIN_SRC latex :exports results 
         \begin{equation}
           \partial u/\partial e = 2Se - 2\lambda e
         \end{equation}
       #+END_SRC
    
       当上式为 0 时，有 $Se=\lambda e$,两边同时除以 $n-1$, 相当于对协方差矩阵求特征向量。
    
       #+BEGIN_QUOTE
       函数 $z=f(x,y)$ 在点 $(x_0,y_0)$ 存在偏导数，且在该点取得极值，则有 $f^{\prime}_x(x_0,y_0)=0$ 和
       $f^{\prime}_y(x_0,y_0)=0$
       #+END_QUOTE
       
       #+BEGIN_QUOTE
       若函数 $z=f(x,y)$ 在点 $(x_0,y_0)$ 的某领域内具有一阶和二阶连续偏导数，且 $f^{\prime}_x(x_0,y_0)=0,
       f^{\prime}_y(x_0,y_0)=0$, 令 $A=f_{xx}(x_0,y_0), B=f_{xy}(x_0,y_0), C=f_{yy}(x_0,y_0)$, 则
       - 当 $AC - B^2>0$ 时，具有极值，且当 $A<0$ 时，取极大， $A<0$ 时取极小
       - 当 $AC - B^2<0$ 时，没有极值
       - 当 $AC - B^2=0$ 时，不能确定
       #+END_QUOTE


       从不同思路出发，得到同一个结果，对协方差矩阵求特征向量，求得后特征向量上就成为了新的坐标，如下图：

       [[file:PCA/least_sq_err_02.png]]
